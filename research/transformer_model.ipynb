{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e737e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f60923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Unversity\\Transformer_KB_Attention\\kb_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# improting pandas for dataframe difference operation\n",
    "import pandas as pd\n",
    "# importing torch.nn.functional module for call method of softmax\n",
    "import torch.nn.functional as F\n",
    "# import pad_sequence for make same length of all sentence tokens during batching\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# import autotokenizer for download pretrained tokenizer, and automodel for download pretrained embed model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# import dataset and dataloader for making custom dataset with addition operations , loader for shuffle , batching\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "# import torch for tensor operations or used for different module\n",
    "import torch\n",
    "# import nn for different deeplearning model like lstm ,linear etc\n",
    "import torch.nn as nn\n",
    "# import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "# importing json for load and dump json file\n",
    "import json\n",
    "# importing csv file for creating file for logging report summary\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "693e8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cuda is available or not \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a2e5f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)  \n",
    "    def forward(self, embed):\n",
    "        query = self.query_w(embed)\n",
    "        key = self.key_w(embed)\n",
    "        value = self.value_w(embed)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** 0.5)\n",
    "        attn_weights = self.softmax(scores)\n",
    "        attended = torch.matmul(attn_weights, value)\n",
    "        return attended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "32169b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_atten = SelfAttention(1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "35dac344",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten = self_atten(input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "489ba4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "315589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.multi_head_attn = nn.ModuleList([\n",
    "            SelfAttention(embed_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.W = nn.Linear(num_heads * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, embed):\n",
    "        heads = [head(embed) for head in self.multi_head_attn]\n",
    "        heads_cat = torch.cat(heads, dim=-1)\n",
    "        output = self.W(heads_cat)\n",
    "        return output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c3bb3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiheadAttention(8, 1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3c0e5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiattent = mha(input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c7bf32f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiattent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e05f1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(embed_dim))  \n",
    "        self.beta = nn.Parameter(torch.zeros(embed_dim))  \n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, embed):\n",
    "        mean = embed.mean(dim=-1, keepdim=True)\n",
    "        var = embed.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized = (embed - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        return self.alpha * normalized + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "178422a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = LayerNormalization(1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "809b0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnorm = norm(multiattent + input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdbd923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W2 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, embed):\n",
    "        x = self.W1(embed)\n",
    "        x = self.dropout(x)\n",
    "        x = self.W2(x)  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "934d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ff = FeedForward(embed_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7605ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = Ff(lnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1eff1dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4c8a118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()  \n",
    "        self.multiheadattention = MultiheadAttention(num_heads, embed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)  \n",
    "        self.feedforward = FeedForward(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atten_x = self.multiheadattention(x)\n",
    "        x = self.layernorm1(atten_x + x)\n",
    "        ff_out = self.feedforward(x)\n",
    "        x = self.layernorm2(ff_out + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "429b6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(8, 1024).to(device)\n",
    "enc_out = enc(input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1d82f83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b9caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ae51836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackEncoder(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(num_heads, embed_dim) for _ in range(6)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoders(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "86b865bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = StackEncoder(8, 1024).to(device)\n",
    "enc_out = enc(input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4c8ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mat(len_seq):\n",
    "    mask_mat = torch.zeros(len_seq, len_seq)\n",
    "    for i in range(len_seq):\n",
    "        for j in range(len_seq):\n",
    "            if i < j:\n",
    "                mask_mat[i][j] = float(\"-inf\")\n",
    "    return mask_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4f58ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = mask_mat(input_embeds.shape[1]).expand(8, -1,-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e363e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_w = nn.Linear(embed_dim, embed_dim) \n",
    "        self.softmax = nn.Softmax(dim=-1)  \n",
    "\n",
    "    def forward(self, embed, mask_mat=None):\n",
    "        query = self.q_w(embed)\n",
    "        key = self.k_w(embed)\n",
    "        value = self.v_w(embed)\n",
    "        atten_score = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** 0.5)\n",
    "         \n",
    "        if mask_mat is not None:\n",
    "            atten_score = atten_score + mask_mat  \n",
    "            \n",
    "        mask_atten_weight = self.softmax(atten_score)\n",
    "        attended = torch.matmul(mask_atten_weight, value)\n",
    "        return attended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d24a4b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA = MaskAttention(1024).to(device)\n",
    "ma = MA(input_embeds, mask_matrix)\n",
    "ma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "92d451fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskMultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.multi_head_attn = nn.ModuleList([\n",
    "            MaskAttention(embed_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.W = nn.Linear(num_heads * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, embed, mask_mat):\n",
    "        heads = [head(embed, mask_mat) for head in self.multi_head_attn]\n",
    "        heads_cat = torch.cat(heads, dim=-1)\n",
    "        output = self.W(heads_cat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f8675153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MMA = MaskMultiheadAttention(8, 1024).to(device)\n",
    "ma = MMA(input_embeds, mask_matrix)\n",
    "ma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c031ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        query = self.q_w(y)\n",
    "        key = self.k_w(x)\n",
    "        value = self.v_w(x)\n",
    "      \n",
    "        atten_score = torch.matmul(query , key.transpose(-2,-1)) / (key.shape[-1] ** 0.5)\n",
    "        atten_weight = self.softmax(atten_score)\n",
    "        attention = torch.matmul(atten_weight, value)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ea087832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CA = CrossAttention(1024).to(device)\n",
    "\n",
    "ca_out=CA(enc_out, ma)\n",
    "ca_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "cf674650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadCrossAttention(nn.Module):\n",
    "    def __init__(self,embed_dim, num_head):\n",
    "        super().__init__()\n",
    "        self.atten_list = nn.ModuleList([CrossAttention(embed_dim) for _ in range(num_head)])\n",
    "        self.W = nn.Linear(embed_dim * num_head, embed_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        heads = [ head(x,y) for head in self.atten_list]\n",
    "        heads_cat = torch.cat(heads, dim=-1)\n",
    "        out = self.W(heads_cat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7940636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCA = MultiheadCrossAttention(1024, 8).to(device)\n",
    "mca_out = MCA(enc_out, ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "3e05a62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mca_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f24b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mask_attention = MaskMultiheadAttention(num_heads, embed_dim)\n",
    "        self.layer_norm_1 = LayerNormalization(embed_dim)\n",
    "        self.cross_attention = MultiheadCrossAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = LayerNormalization(embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.layer_norm_3 = LayerNormalization(embed_dim)\n",
    "\n",
    "    def forward(self, x, y, mask_mat):\n",
    "        mask_atten = self.mask_attention(y, mask_mat)\n",
    "        y_norm = self.layer_norm_1(mask_atten + y)\n",
    "        cross_atten = self.cross_attention(x, y_norm)\n",
    "        cross_norm = self.layer_norm_2(cross_atten + x)\n",
    "        ff_out = self.feed_forward(cross_norm)\n",
    "        out_norm = self.layer_norm_3(ff_out + cross_norm)\n",
    "        return out_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "468c34d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(1024, 8).to(device)\n",
    "decoder(enc_out, input_embeds, mask_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2fc4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.decoders = nn.ModuleList([\n",
    "            Decoder(embed_dim, num_heads) for _ in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, y, mask_mat):\n",
    "        for decoder in self.decoders:\n",
    "            y = decoder(x, y, mask_mat)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "97e452a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderStack(1024, 8).to(device)\n",
    "decoder(enc_out, input_embeds, mask_matrix).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ca2ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_position(p, i, dim):\n",
    "    return math.sin(p / (10000 ** ((2 * i) / dim)))\n",
    "\n",
    "def odd_position(p, i, dim):\n",
    "    return math.cos(p / (10000 ** ((2 * i) / dim)))\n",
    "\n",
    "def positional_encoding(tokens_len, embed_dim):\n",
    "    positional_encodings = []\n",
    "    for p in range(tokens_len):\n",
    "        token_position = []\n",
    "        for i in range(embed_dim):\n",
    "            if i % 2 == 0:\n",
    "                token_position.append(even_position(p, i, embed_dim))\n",
    "            else:\n",
    "                token_position.append(odd_position(p, i, embed_dim))\n",
    "        positional_encodings.append(torch.tensor(token_position))\n",
    "    return torch.stack(positional_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54018af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(10, 1024).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e93c692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = StackEncoder(num_heads, embed_dim)\n",
    "        self.decoders = DecoderStack(embed_dim, num_heads)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, y, mask_mat, postional_encoding):\n",
    "        x = x + postional_encoding\n",
    "        y = y + postional_encoding\n",
    "        x = self.encoder(x)\n",
    "        y = self.decoders(x,y, mask_mat)\n",
    "        out_logits = self.linear(y)\n",
    "        return out_logits, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8e690693",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerBlock(8, 1024, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0990a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, y = transformer(input_embeds, input_embeds, mask_matrix, pos_encod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e0ad1f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9, 1024])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4c51320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerKBAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.transformers = TransformerBlock(num_heads, embed_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.W_kb1 = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def kb_attention(self, decoder_hidden, kb_keys, new2old):\n",
    "        \"\"\"\n",
    "        decoder_hidden: [B, T, D]\n",
    "        kb_keys: [B, K, D]\n",
    "        new2old: list of length K, mapping from kb_key idx → vocab idx\n",
    "        \"\"\"\n",
    "        B, T, D = decoder_hidden.shape\n",
    "        K = kb_keys.shape[1]\n",
    "\n",
    "        # Project decoder hidden states\n",
    "        x = torch.tanh(self.W_kb1(decoder_hidden))         # [B, T, D]\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        x_norm = F.normalize(x, p=2, dim=-1)               # [B, T, D]\n",
    "        kb_key_norm = F.normalize(kb_keys, p=2, dim=-1)    # [B, K, D]\n",
    "\n",
    "        # Compute cosine similarity between each decoder token and each KB key\n",
    "        # [B, T, K]\n",
    "        cosine_sim = torch.matmul(x_norm, kb_key_norm.transpose(1, 2))  \n",
    "\n",
    "        # Initialize attention tensor: [B, T, vocab_size]\n",
    "        kb_attention = torch.zeros(B, T, self.vocab_size, device=decoder_hidden.device)\n",
    "\n",
    "        # Loop over each KB key index\n",
    "        for k_idx, vocab_idx in enumerate(new2old):\n",
    "            # cosine_sim[:, :, k_idx] → [B, T], similarity for this KB key\n",
    "            kb_attention[:, :, vocab_idx] += cosine_sim[:, :, k_idx]\n",
    "\n",
    "        return kb_attention\n",
    "\n",
    "    \n",
    "    def forward(self,  x, y, mask_mat, positional_encod, new2old, kb_keys):\n",
    "        vocab_logit, dec_out = self.transformers(x,y, mask_mat, positional_encod)\n",
    "        kb_atten = self.kb_attention(dec_out, kb_keys=kb_keys, new2old=new2old)\n",
    "        logits = vocab_logit + kb_atten\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b2c75022",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkb = TransformerKBAttention(8, 1024, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c4e1bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_embed = keys_embed.expand(8, -1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "65ab723f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 492, 1024])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "db426fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2877,  0.2992, -0.0485,  ...,  0.0439, -0.2981, -0.0910],\n",
       "         [-0.7821, -0.6512,  0.2742,  ..., -0.2105, -0.8132,  0.2286],\n",
       "         [-0.7546, -0.1569,  0.1842,  ...,  0.0055, -0.2678,  0.5550],\n",
       "         ...,\n",
       "         [-1.1172, -0.0050, -0.0065,  ...,  0.4781, -0.6678,  0.7323],\n",
       "         [-0.8320, -0.1472, -0.1423,  ..., -0.3938, -0.8935,  0.8311],\n",
       "         [-0.6768, -0.0465,  0.1754,  ...,  0.0143, -1.2519,  1.0366]],\n",
       "\n",
       "        [[ 0.5688,  0.3594,  0.0303,  ..., -0.0431,  0.1983, -0.4620],\n",
       "         [-0.3962,  0.0097, -0.5491,  ..., -0.5772, -0.4538, -0.2825],\n",
       "         [ 0.0283,  0.0408, -0.5234,  ...,  0.0177,  0.4672,  0.2835],\n",
       "         ...,\n",
       "         [-0.2635,  0.6658, -0.3592,  ...,  0.1436, -1.3863,  1.1012],\n",
       "         [ 0.0203,  0.2050, -0.1696,  ..., -0.1309, -1.2625,  0.7859],\n",
       "         [ 0.1673,  0.0696, -0.3251,  ..., -0.4303, -1.0181,  0.8010]],\n",
       "\n",
       "        [[ 0.0854,  0.3682,  0.2540,  ...,  0.3005, -0.1219, -0.1078],\n",
       "         [-0.7070,  0.4751,  0.7104,  ...,  0.1931, -1.0400,  0.6888],\n",
       "         [-0.1281,  0.6263,  0.1314,  ...,  0.6162, -0.5934,  0.1836],\n",
       "         ...,\n",
       "         [-0.3591,  0.0756,  0.1940,  ...,  0.3555, -1.1727,  0.8464],\n",
       "         [-0.7782,  0.3019,  0.8770,  ...,  0.6883, -0.8815,  0.8250],\n",
       "         [-0.3762,  0.2051,  0.6874,  ..., -0.2348, -1.1282,  0.8316]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7882,  0.3612,  0.3484,  ..., -0.2698, -0.5036, -0.1543],\n",
       "         [ 0.6330,  0.8750, -0.2353,  ..., -0.0804, -0.5834,  0.0242],\n",
       "         [ 0.1376,  0.5839,  0.3117,  ..., -0.3230, -0.1864,  0.3712],\n",
       "         ...,\n",
       "         [-0.1463,  0.3362, -1.0080,  ..., -0.1249,  0.2085,  0.0575],\n",
       "         [-0.3480, -0.0388,  0.2429,  ...,  0.5548,  0.0279,  0.0793],\n",
       "         [-0.2670, -0.3342, -0.3276,  ..., -0.1766, -1.6365,  0.7495]],\n",
       "\n",
       "        [[ 0.1599,  0.4397,  0.0577,  ..., -0.0834, -0.1321, -0.1690],\n",
       "         [-0.1477,  0.8021,  0.3773,  ...,  0.1790, -0.9301, -0.1023],\n",
       "         [-0.2539,  0.9896,  0.1686,  ..., -0.1977, -0.2682,  0.2933],\n",
       "         ...,\n",
       "         [-0.3749,  0.5034,  0.4826,  ..., -0.0649, -0.9961, -0.0780],\n",
       "         [-0.2151, -0.4204,  0.3071,  ...,  0.1796, -1.5402,  1.0861],\n",
       "         [-0.1642,  0.0885,  0.3538,  ...,  0.0685, -1.8010,  1.1462]],\n",
       "\n",
       "        [[ 0.7007,  0.2624, -0.6226,  ..., -0.1739,  0.1678, -0.4906],\n",
       "         [-0.6718, -0.0471,  0.0883,  ..., -0.7363, -1.3150,  0.8355],\n",
       "         [-0.9368, -0.2642,  0.4170,  ...,  0.3176, -0.0527,  0.4305],\n",
       "         ...,\n",
       "         [-0.7205, -0.2598, -0.2053,  ..., -0.5897, -0.1240,  0.2608],\n",
       "         [-0.3781, -0.6073,  0.0481,  ...,  0.2380, -0.7997,  1.0202],\n",
       "         [-0.9312, -0.5729,  0.1235,  ..., -0.1534, -0.8878,  0.9769]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkb(input_embeds, input_embeds, mask_matrix, pos_encod, new2old, keys_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fbd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7796d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocess\n",
    "def preprocess_data(text):\n",
    "    # lowering the text\n",
    "    text = text.lower()\n",
    "    # replace \"?\" with ''\n",
    "    text = text.replace('?','')\n",
    "    # replace \"'\" with ''\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    # replace \",\" with ''\n",
    "    text = text.replace(\",\",\" \")\n",
    "    # replace \"1)\" with ''\n",
    "    text = text.replace(\"1)\",\" \")\n",
    "    # replace \"2)\" with ''\n",
    "    text = text.replace(\"2)\",\" \")\n",
    "    # replace \"3)\" with ''\n",
    "    text = text.replace(\"3)\",\" \")\n",
    "    # replace \"4)\" with ''\n",
    "    text = text.replace(\"4)\",\" \")\n",
    "    # replace \".\" with ''\n",
    "    text = text.replace(\".\",\" \")\n",
    "    # strip replace white space from forward and backward\n",
    "    text = text.strip()\n",
    "    # return preprcess text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f5780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained tokenizer qwen model form hugging face\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\n",
    "# download pretrained embedding qwen model form hugging face\n",
    "embed_model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a11e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    # initilze data and maxlength of tokens\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.max_length = 8192\n",
    "\n",
    "    def __len__(self):\n",
    "        # here retun len of data\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get one by one 'patent query' from data stored in questions\n",
    "        question = self.data.iloc[index]['Patient query']\n",
    "        # get one by 'doctor response' from data stored in questions\n",
    "        answer = self.data.iloc[index]['Doctor response']\n",
    "        \n",
    "        # get token id from questin text , also set return tensor 'pt'\n",
    "        question_ids = tokenizer(\n",
    "            question,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'][0] \n",
    "        \n",
    "        # get token id from anwer  text , also set return tensor 'pt'\n",
    "        answer_ids = tokenizer(\n",
    "            answer,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'][0]\n",
    "    \n",
    "        return question_ids, answer_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e4f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open appointmenst_data in read mode\n",
    "with open('../data/appointments_data.json', 'r') as f:\n",
    "    # load that json data into key value data\n",
    "    key_value_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84d4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open qa pair.json in read mode\n",
    "with open('../data/qa_pairs.json' , 'r') as f:\n",
    "    # load json data into db_data\n",
    "    db_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f8a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess db data q/a and stored in patient and assistant\n",
    "patient , assistant = [ preprocess_data(i['question']) for i in db_data['Q&A'] ], [ preprocess_data(i['answer']) for i in db_data['Q&A'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74bd045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize new key value data for stored preprocessed key value data\n",
    "new_key_value_data = {}\n",
    "# iterating kv data through items functions to get key value \n",
    "for key, value in key_value_data.items():\n",
    "    # preprocess key \n",
    "    new_key = preprocess_data(key)\n",
    "    # preprocess value\n",
    "    new_value = preprocess_data(value)\n",
    "    # add data into new key value data\n",
    "    new_key_value_data[new_key] = new_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2abdeb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mapping functions to map values with key for canonical tokens\n",
    "def replace_values_with_keys(texts, mapping):\n",
    "    # enumeratiing text list to get index and values\n",
    "    for i, text in enumerate(texts):\n",
    "        # from mapping data to get key an vlues\n",
    "        for key, val in mapping.items():\n",
    "            # split key by '_' and take only first text\n",
    "            name = key.split('_')[0]\n",
    "            # check name is present in text or not and also val present in text or not if both condition true then only enter inside if condition\n",
    "            if name in text and val in text:\n",
    "                # condition true , replace val with key\n",
    "                texts[i] = text.replace(val, key)\n",
    "                # break \n",
    "                break\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67382659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call replace values with keys with assistan and newkeyvalue data parameters\n",
    "assistant = replace_values_with_keys(assistant, new_key_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f12b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension to get all keys from dict\n",
    "all_keys = [ i for i in new_key_value_data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1cba255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add all keys to tokens\n",
    "tokenizer.add_tokens(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f82e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(152161, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize token token embedding after increase len of tokenizer\n",
    "embed_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5e17b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize set for storing used token ids \n",
    "used_token_ids = set()\n",
    "\n",
    "# iterating all keys\n",
    "for target_text in all_keys:\n",
    "    # encode token to tokens id with add special tokens false\n",
    "    tokens = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    # update the used token ids \n",
    "    used_token_ids.update(tokens)\n",
    "\n",
    "# sort the used token ids by its token ids\n",
    "token_id_list = sorted(list(used_token_ids))\n",
    "\n",
    "# used dict comprehession , apply enumerate to get idx and values \n",
    "# here assign value as key and idx as value for old to new \n",
    "old2new = {old: new for new, old in enumerate(token_id_list)}\n",
    "# here also use dict comprehession for exchange key to value and value to key.\n",
    "new2old = {v: k for k, v in old2new.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "875b957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient query</th>\n",
       "      <th>Doctor response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what time is benjamins appointment</td>\n",
       "      <td>the appointment time of benjamin is benjamin_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who is michaels doctor</td>\n",
       "      <td>the doctor for michael is michael_doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the contact number for kenneth</td>\n",
       "      <td>the contact number for kenneth is kenneth_contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what time is blakes appointment</td>\n",
       "      <td>the appointment time of blake is blake_appoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where is brenda having their appointment</td>\n",
       "      <td>brenda is having their appointment at brenda_h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Patient query  \\\n",
       "0        what time is benjamins appointment   \n",
       "1                    who is michaels doctor   \n",
       "2    what is the contact number for kenneth   \n",
       "3           what time is blakes appointment   \n",
       "4  where is brenda having their appointment   \n",
       "\n",
       "                                     Doctor response  \n",
       "0  the appointment time of benjamin is benjamin_a...  \n",
       "1           the doctor for michael is michael_doctor  \n",
       "2  the contact number for kenneth is kenneth_contact  \n",
       "3  the appointment time of blake is blake_appoint...  \n",
       "4  brenda is having their appointment at brenda_h...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dictionery db_data_dict having key \"Patient query\" and \"Doctor response\" with list of vlaues patient and doctor\n",
    "db_data_dict = {'Patient query': patient, 'Doctor response': assistant}\n",
    "print(len(patient))\n",
    "# create data frame db_data_dict\n",
    "db_df = pd.DataFrame(db_data_dict)\n",
    "# call head method of df, to get five data rows\n",
    "db_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78e304fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply prepocess text on patient query and doctor response\n",
    "db_df['Patient query'] = db_df['Patient query'].apply(preprocess_data)\n",
    "db_df['Doctor response'] = db_df['Doctor response'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e294f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = CustomDataset(db_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9b8a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, val sets\n",
    "train_set, val_set = torch.utils.data.random_split(dataset_db, [3500, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dba6440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    questions, answers = zip(*batch) \n",
    "\n",
    "    padded_questions = pad_sequence(questions, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_answers = pad_sequence(answers, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return padded_questions, padded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "325db718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_db = DataLoader(train_set , batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader_db = DataLoader(val_set , batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbc663e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for x, y in train_loader_db:\n",
    "    x = x\n",
    "    y = y\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8828a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embed model into device , it may be cuda or cpu\n",
    "embed_model = embed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdefe440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize key embed list\n",
    "keys_embed = []\n",
    "# iterate all keys\n",
    "for key in all_keys:\n",
    "    # tokenize each key to get token id\n",
    "    key = tokenizer(key, return_tensors='pt')['input_ids'][:,:-1]\n",
    "    # embed tokenize key id\n",
    "    key_embed = embed_model(key.to(device), )['last_hidden_state'][0]\n",
    "    # append key embed to keys_embed list\n",
    "    keys_embed.append(key_embed)\n",
    "\n",
    "# stack keys_embed list on dim=0 , apply squeeze to decrease dimension on axis=2, \n",
    "# detach for autograd false\n",
    "keys_embed = torch.stack(keys_embed, dim=0).squeeze(1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d06880e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize start tokens\n",
    "start_token = tokenizer('<|im_start|>', return_tensors='pt')['input_ids'][:,0].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fafbbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(15, 1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "267860f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "446b2932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding[:x_len].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14f27925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1b2f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88b0f0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 1024])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encod.expand(8, -1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eb461f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 9])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SelfAttention(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader_db:\n",
    "    pos_encod = pos_encoding[:x.shape[-1]]\n",
    "    pos_encod = pos_encod.expand(8, -1, -1)\n",
    "    with torch.no_grad():\n",
    "            input_embeds = embed_model(x.to(device))['last_hidden_state'].detach()\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f4c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0beea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46372169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacd01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85b168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5ebb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e55028",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaskAttention.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# initialize model with (embed_model, vocab-size, hidden dim , embedding )\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, num_heads, embed_dim, vocab_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m StackEncoder(num_heads, embed_dim, embed_dim)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, vocab_size)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m, in \u001b[0;36mDecoderStack.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      5\u001b[0m         Decoder(embed_dim, num_heads) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_attention \u001b[38;5;241m=\u001b[39m \u001b[43mMaskMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1 \u001b[38;5;241m=\u001b[39m LayerNormalization(embed_dim)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention \u001b[38;5;241m=\u001b[39m MultiheadCrossAttention(embed_dim, num_heads)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mMaskMultiheadAttention.__init__\u001b[1;34m(self, num_heads, embed_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      8\u001b[0m     MaskAttention(embed_dim,embed_dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mMaskAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: MaskAttention.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# get vocabsize\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "# intialize num of head\n",
    "num_head = 8\n",
    "# initialize embed_dim\n",
    "embed_dim = 1024\n",
    "# initialize model with (embed_model, vocab-size, hidden dim , embedding )\n",
    "model = TransformerBlock(num_heads=num_head, embed_dim=embed_dim, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "512853e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaskAttention.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 5\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, num_heads, embed_dim, vocab_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m StackEncoder(num_heads, embed_dim, embed_dim)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, vocab_size)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m, in \u001b[0;36mDecoderStack.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      5\u001b[0m         Decoder(embed_dim, num_heads) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_attention \u001b[38;5;241m=\u001b[39m \u001b[43mMaskMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1 \u001b[38;5;241m=\u001b[39m LayerNormalization(embed_dim)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention \u001b[38;5;241m=\u001b[39m MultiheadCrossAttention(embed_dim, num_heads)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mMaskMultiheadAttention.__init__\u001b[1;34m(self, num_heads, embed_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      8\u001b[0m     MaskAttention(embed_dim,embed_dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mMaskAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: MaskAttention.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "TransformerBlock(num_heads=num_head, embed_dim=embed_dim, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383daeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
