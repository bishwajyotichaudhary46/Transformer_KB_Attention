{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e737e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f60923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Unversity\\Transformer_KB_Attention\\kb_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# improting pandas for dataframe difference operation\n",
    "import pandas as pd\n",
    "# importing torch.nn.functional module for call method of softmax\n",
    "import torch.nn.functional as F\n",
    "# import pad_sequence for make same length of all sentence tokens during batching\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# import autotokenizer for download pretrained tokenizer, and automodel for download pretrained embed model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# import dataset and dataloader for making custom dataset with addition operations , loader for shuffle , batching\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "# import torch for tensor operations or used for different module\n",
    "import torch\n",
    "# import nn for different deeplearning model like lstm ,linear etc\n",
    "import torch.nn as nn\n",
    "# import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "# importing json for load and dump json file\n",
    "import json\n",
    "# importing csv file for creating file for logging report summary\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693e8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cuda is available or not \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e5f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)  \n",
    "    def forward(self, embed):\n",
    "        query = self.query_w(embed)\n",
    "        key = self.key_w(embed)\n",
    "        value = self.value_w(embed)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** 0.5)\n",
    "        attn_weights = self.softmax(scores)\n",
    "        attended = torch.matmul(attn_weights, value)\n",
    "        return attended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315589f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.multi_head_attn = nn.ModuleList([\n",
    "            SelfAttention(embed_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.W = nn.Linear(num_heads * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, embed):\n",
    "        heads = [head(embed) for head in self.multi_head_attn]\n",
    "        heads_cat = torch.cat(heads, dim=-1)\n",
    "        output = self.W(heads_cat)\n",
    "        return output\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e05f1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(embed_dim))  \n",
    "        self.beta = nn.Parameter(torch.zeros(embed_dim))  \n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, embed):\n",
    "        mean = embed.mean(dim=-1, keepdim=True)\n",
    "        var = embed.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        normalized = (embed - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        return self.alpha * normalized + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdbd923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W2 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, embed):\n",
    "        x = self.W1(embed)\n",
    "        x = self.dropout(x)\n",
    "        x = self.W2(x)  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8a118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()  \n",
    "        self.multiheadattention = MultiheadAttention(num_heads, embed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)  \n",
    "        self.feedforward = FeedForward(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        atten_x = self.multiheadattention(x)\n",
    "        x = self.layernorm1(atten_x + x)\n",
    "        ff_out = self.feedforward(x)\n",
    "        x = self.layernorm2(ff_out + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae51836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackEncoder(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[Encoder(num_heads, embed_dim) for _ in range(6)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoders(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c8ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mat(len_seq):\n",
    "    mask_mat = torch.zeros(len_seq, len_seq)\n",
    "    for i in range(len_seq):\n",
    "        for j in range(len_seq):\n",
    "            if i < j:\n",
    "                mask_mat[i][j] = float(\"-inf\")\n",
    "    return mask_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e363e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_w = nn.Linear(embed_dim, embed_dim) \n",
    "        self.softmax = nn.Softmax(dim=-1)  \n",
    "\n",
    "    def forward(self, embed, mask_mat=None):\n",
    "        query = self.q_w(embed)\n",
    "        key = self.k_w(embed)\n",
    "        value = self.v_w(embed)\n",
    "        atten_score = torch.matmul(query, key.transpose(-2, -1)) / (key.shape[-1] ** 0.5)\n",
    "         \n",
    "        if mask_mat is not None:\n",
    "            atten_score = atten_score + mask_mat  \n",
    "            \n",
    "        mask_atten_weight = self.softmax(atten_score)\n",
    "        attended = torch.matmul(mask_atten_weight, value)\n",
    "        return attended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92d451fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskMultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.multi_head_attn = nn.ModuleList([\n",
    "            MaskAttention(embed_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.W = nn.Linear(num_heads * embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, embed, mask_mat):\n",
    "        heads = [head(embed, mask_mat) for head in self.multi_head_attn]\n",
    "        heads_cat = torch.cat(heads, dim=-1)\n",
    "        output = self.W(heads_cat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.q_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_w = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        query = self.q_w(y)\n",
    "        key = self.k_w(x)\n",
    "        value = self.v_w(x)\n",
    "        atten_score = torch.matmul(query , key.transpose(-2,-1)) / (key.shape[-1] ** 0.5)\n",
    "        atten_weight = self.softmax(atten_score)\n",
    "        attention = torch.matmul(atten_weight, value)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf674650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadCrossAttention(nn.Module):\n",
    "    def __init__(self,embed_dim, num_head):\n",
    "        super().__init__()\n",
    "        self.atten_list = nn.ModuleList([CrossAttention(embed_dim) for _ in range(num_head)])\n",
    "        self.W = nn.Linear(embed_dim * num_head, embed_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        heads = [ head(x,y) for head in self.atten_list]\n",
    "        heads_cat = torch.cat(heads, dim=-1)\n",
    "        out = self.W(heads_cat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f24b1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mask_attention = MaskMultiheadAttention(num_heads, embed_dim)\n",
    "        self.layer_norm_1 = LayerNormalization(embed_dim)\n",
    "        self.cross_attention = MultiheadCrossAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = LayerNormalization(embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim)\n",
    "        self.layer_norm_3 = LayerNormalization(embed_dim)\n",
    "\n",
    "    def forward(self, x, y, mask_mat):\n",
    "        mask_atten = self.mask_attention(y, mask_mat)\n",
    "        y_norm = self.layer_norm_1(mask_atten + y)\n",
    "        cross_atten = self.cross_attention(x, y_norm)\n",
    "        cross_norm = self.layer_norm_2(cross_atten + y)\n",
    "        ff_out = self.feed_forward(cross_norm)\n",
    "        out_norm = self.layer_norm_3(ff_out + cross_norm)\n",
    "        return out_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2fc4fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.decoders = nn.ModuleList([\n",
    "            Decoder(embed_dim, num_heads) for _ in range(6)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, y, mask_mat):\n",
    "        for decoder in self.decoders:\n",
    "            y = decoder(x, y, mask_mat)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6ca2ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def even_position(p, i, dim):\n",
    "    return math.sin(p / (10000 ** ((2 * i) / dim)))\n",
    "\n",
    "def odd_position(p, i, dim):\n",
    "    return math.cos(p / (10000 ** ((2 * i) / dim)))\n",
    "\n",
    "def positional_encoding(tokens_len, embed_dim):\n",
    "    positional_encodings = []\n",
    "    for p in range(tokens_len):\n",
    "        token_position = []\n",
    "        for i in range(embed_dim):\n",
    "            if i % 2 == 0:\n",
    "                token_position.append(even_position(p, i, embed_dim))\n",
    "            else:\n",
    "                token_position.append(odd_position(p, i, embed_dim))\n",
    "        positional_encodings.append(torch.tensor(token_position))\n",
    "    return torch.stack(positional_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e93c692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = StackEncoder(num_heads, embed_dim)\n",
    "        self.decoders = DecoderStack(embed_dim, num_heads)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, y, mask_mat, postional_encoding_x, pos_encod_y):\n",
    "        x = x + postional_encoding_x\n",
    "        y = y + pos_encod_y\n",
    "        x = self.encoder(x)\n",
    "        y = self.decoders(x,y, mask_mat)\n",
    "        out_logits = self.linear(y)\n",
    "        return out_logits, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerKBAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.transformers = TransformerBlock(num_heads, embed_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.W_kb1 = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def kb_attention(self, decoder_hidden, kb_keys, new2old):\n",
    "        \"\"\"\n",
    "        decoder_hidden: [B, T, D]\n",
    "        kb_keys: [B, K, D]\n",
    "        new2old: list of length K, mapping from kb_key idx → vocab idx\n",
    "        \"\"\"\n",
    "        B, T, D = decoder_hidden.shape\n",
    "        K = kb_keys.shape[1]\n",
    "\n",
    "        # Project decoder hidden states\n",
    "        x = torch.tanh(self.W_kb1(decoder_hidden))         # [B, T, D]\n",
    "\n",
    "        # Normalize for cosine similarity\n",
    "        x_norm = F.normalize(x, p=2, dim=-1)               # [B, T, D]\n",
    "        kb_key_norm = F.normalize(kb_keys, p=2, dim=-1)    # [B, K, D]\n",
    "\n",
    "        # Compute cosine similarity between each decoder token and each KB key\n",
    "        # [B, T, K]\n",
    "        cosine_sim = torch.matmul(x_norm, kb_key_norm.transpose(1, 2))  \n",
    "\n",
    "\n",
    "        # Initialize attention tensor: [B, T, vocab_size]\n",
    "        kb_attention = torch.zeros(B, T, self.vocab_size, device=decoder_hidden.device)\n",
    "\n",
    "        # Loop over each KB key index\n",
    "        for k_idx, vocab_idx in new2old.items():\n",
    "            kb_attention[:, :, vocab_idx] = cosine_sim[:, :, k_idx]\n",
    "            \n",
    "        return kb_attention\n",
    "\n",
    "    \n",
    "    def forward(self,  x, y, mask_mat, positional_encod_x, pos_encod_y, new2old, kb_keys):\n",
    "        vocab_logit, dec_out = self.transformers(x,y, mask_mat, positional_encod_x, pos_encod_y)\n",
    "        kb_atten = self.kb_attention(dec_out, kb_keys=kb_keys, new2old=new2old)\n",
    "        logits = vocab_logit + kb_atten\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed7796d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocess\n",
    "def preprocess_data(text):\n",
    "    # lowering the text\n",
    "    text = text.lower()\n",
    "    # replace \"?\" with ''\n",
    "    text = text.replace('?','')\n",
    "    # replace \"'\" with ''\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    # replace \",\" with ''\n",
    "    text = text.replace(\",\",\" \")\n",
    "    # replace \"1)\" with ''\n",
    "    text = text.replace(\"1)\",\" \")\n",
    "    # replace \"2)\" with ''\n",
    "    text = text.replace(\"2)\",\" \")\n",
    "    # replace \"3)\" with ''\n",
    "    text = text.replace(\"3)\",\" \")\n",
    "    # replace \"4)\" with ''\n",
    "    text = text.replace(\"4)\",\" \")\n",
    "    # replace \".\" with ''\n",
    "    text = text.replace(\".\",\" \")\n",
    "    # strip replace white space from forward and backward\n",
    "    text = text.strip()\n",
    "    # return preprcess text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4f5780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained tokenizer qwen model form hugging face\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\n",
    "# download pretrained embedding qwen model form hugging face\n",
    "embed_model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a11e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    # initilze data and maxlength of tokens\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.max_length = 8192\n",
    "\n",
    "    def __len__(self):\n",
    "        # here retun len of data\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get one by one 'patent query' from data stored in questions\n",
    "        question = self.data.iloc[index]['Patient query']\n",
    "        # get one by 'doctor response' from data stored in questions\n",
    "        answer = self.data.iloc[index]['Doctor response']\n",
    "        \n",
    "        # get token id from questin text , also set return tensor 'pt'\n",
    "        question_ids = tokenizer(\n",
    "            question,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'][0] \n",
    "        \n",
    "        # get token id from anwer  text , also set return tensor 'pt'\n",
    "        answer_ids = tokenizer(\n",
    "            answer,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'][0]\n",
    "    \n",
    "        return question_ids, answer_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76e4f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open appointmenst_data in read mode\n",
    "with open('../data/appointments_data.json', 'r') as f:\n",
    "    # load that json data into key value data\n",
    "    key_value_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b84d4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open qa pair.json in read mode\n",
    "with open('../data/qa_pairs.json' , 'r') as f:\n",
    "    # load json data into db_data\n",
    "    db_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7f8a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess db data q/a and stored in patient and assistant\n",
    "patient , assistant = [ preprocess_data(i['question']) for i in db_data['Q&A'] ], [ preprocess_data(i['answer']) for i in db_data['Q&A'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74bd045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize new key value data for stored preprocessed key value data\n",
    "new_key_value_data = {}\n",
    "# iterating kv data through items functions to get key value \n",
    "for key, value in key_value_data.items():\n",
    "    # preprocess key \n",
    "    new_key = preprocess_data(key)\n",
    "    # preprocess value\n",
    "    new_value = preprocess_data(value)\n",
    "    # add data into new key value data\n",
    "    new_key_value_data[new_key] = new_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2abdeb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mapping functions to map values with key for canonical tokens\n",
    "def replace_values_with_keys(texts, mapping):\n",
    "    # enumeratiing text list to get index and values\n",
    "    for i, text in enumerate(texts):\n",
    "        # from mapping data to get key an vlues\n",
    "        for key, val in mapping.items():\n",
    "            # split key by '_' and take only first text\n",
    "            name = key.split('_')[0]\n",
    "            # check name is present in text or not and also val present in text or not if both condition true then only enter inside if condition\n",
    "            if name in text and val in text:\n",
    "                # condition true , replace val with key\n",
    "                texts[i] = text.replace(val, key)\n",
    "                # break \n",
    "                break\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67382659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call replace values with keys with assistan and newkeyvalue data parameters\n",
    "assistant = replace_values_with_keys(assistant, new_key_value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f12b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension to get all keys from dict\n",
    "all_keys = [ i for i in new_key_value_data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1cba255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add all keys to tokens\n",
    "tokenizer.add_tokens(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f82e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(152161, 1024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize token token embedding after increase len of tokenizer\n",
    "embed_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5e17b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize set for storing used token ids \n",
    "used_token_ids = set()\n",
    "\n",
    "# iterating all keys\n",
    "for target_text in all_keys:\n",
    "    # encode token to tokens id with add special tokens false\n",
    "    tokens = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    # update the used token ids \n",
    "    used_token_ids.update(tokens)\n",
    "\n",
    "# sort the used token ids by its token ids\n",
    "token_id_list = sorted(list(used_token_ids))\n",
    "\n",
    "# used dict comprehession , apply enumerate to get idx and values \n",
    "# here assign value as key and idx as value for old to new \n",
    "old2new = {old: new for new, old in enumerate(token_id_list)}\n",
    "# here also use dict comprehession for exchange key to value and value to key.\n",
    "new2old = {v: k for k, v in old2new.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "875b957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient query</th>\n",
       "      <th>Doctor response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what time is benjamins appointment</td>\n",
       "      <td>the appointment time of benjamin is benjamin_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who is michaels doctor</td>\n",
       "      <td>the doctor for michael is michael_doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is the contact number for kenneth</td>\n",
       "      <td>the contact number for kenneth is kenneth_contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what time is blakes appointment</td>\n",
       "      <td>the appointment time of blake is blake_appoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where is brenda having their appointment</td>\n",
       "      <td>brenda is having their appointment at brenda_h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Patient query  \\\n",
       "0        what time is benjamins appointment   \n",
       "1                    who is michaels doctor   \n",
       "2    what is the contact number for kenneth   \n",
       "3           what time is blakes appointment   \n",
       "4  where is brenda having their appointment   \n",
       "\n",
       "                                     Doctor response  \n",
       "0  the appointment time of benjamin is benjamin_a...  \n",
       "1           the doctor for michael is michael_doctor  \n",
       "2  the contact number for kenneth is kenneth_contact  \n",
       "3  the appointment time of blake is blake_appoint...  \n",
       "4  brenda is having their appointment at brenda_h...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dictionery db_data_dict having key \"Patient query\" and \"Doctor response\" with list of vlaues patient and doctor\n",
    "db_data_dict = {'Patient query': patient, 'Doctor response': assistant}\n",
    "print(len(patient))\n",
    "# create data frame db_data_dict\n",
    "db_df = pd.DataFrame(db_data_dict)\n",
    "# call head method of df, to get five data rows\n",
    "db_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78e304fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply prepocess text on patient query and doctor response\n",
    "db_df['Patient query'] = db_df['Patient query'].apply(preprocess_data)\n",
    "db_df['Doctor response'] = db_df['Doctor response'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "4e294f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_db = CustomDataset(db_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b8a883",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# split data into train, val sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_set, val_set \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(dataset_db, [\u001b[38;5;241m3600\u001b[39m, \u001b[38;5;241m496\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# split data into train, val sets\n",
    "train_set, val_set = torch.utils.data.random_split(dataset_db, [3600, 496])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2edb6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "dba6440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    questions, answers = zip(*batch) \n",
    "\n",
    "    padded_questions = pad_sequence(questions, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_answers = pad_sequence(answers, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return padded_questions, padded_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "325db718",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_db = DataLoader(train_set , batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader_db = DataLoader(val_set , batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "fbc663e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "for x, y in train_loader_db:\n",
    "    x = x\n",
    "    y = y\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c8828a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embed model into device , it may be cuda or cpu\n",
    "embed_model = embed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "cdefe440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize key embed list\n",
    "keys_embed = []\n",
    "# iterate all keys\n",
    "for key in all_keys:\n",
    "    # tokenize each key to get token id\n",
    "    key = tokenizer(key, return_tensors='pt')['input_ids'][:,:-1]\n",
    "    # embed tokenize key id\n",
    "    key_embed = embed_model(key.to(device), )['last_hidden_state'][0]\n",
    "    # append key embed to keys_embed list\n",
    "    keys_embed.append(key_embed)\n",
    "\n",
    "# stack keys_embed list on dim=0 , apply squeeze to decrease dimension on axis=2, \n",
    "# detach for autograd false\n",
    "keys_embed = torch.stack(keys_embed, dim=0).squeeze(1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "873aaeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([492, 1024])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "04f3b1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([492, 1024])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d06880e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize start tokens\n",
    "start_token = tokenizer('<|im_start|>', return_tensors='pt')['input_ids'][:,0].unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "fafbbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(15, 1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "324e78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = mask_mat(15).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4a6a354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88, 1024])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader_db:\n",
    "    B, T = x.shape\n",
    "    B_y, T_y = y.shape\n",
    "    pos_encod_x = pos_encoding[:T].expand(B, -1, -1)\n",
    "    pos_encod_y = pos_encoding[:T_y+1].expand(B_y, -1, -1)\n",
    "    mask_y = mask_matrix[:T_y+1, :T_y+1].expand(B_y, -1, -1)\n",
    "    y = y.to(device)\n",
    "    start_token_expanded = start_token.expand(y.size(0), 1)\n",
    "    # concat sos token in target tokens\n",
    "    target_with_sos = torch.cat([start_token_expanded, y], dim=1)\n",
    "    with torch.no_grad():\n",
    "            input_embeds = embed_model(x.to(device))['last_hidden_state'].detach()\n",
    "            target_embeds = [embed_model(i.unsqueeze(0).unsqueeze(0))['last_hidden_state'].detach()[0][0]  for target_tokens in target_with_sos for i in target_tokens]\n",
    "            target_embeds = torch.stack(target_embeds)\n",
    "            print(target_embeds.shape)\n",
    "            target_embeds = target_embeds.reshape(B_y, T_y+1, 1024)\n",
    "            \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6b9d7fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 11, 1024])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "cb37f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "model = TransformerKBAttention(num_heads=8, embed_dim=1024, vocab_size=vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "81868a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate \n",
    "learning_rate = 0.001\n",
    "# set epochs\n",
    "epochs = 25\n",
    "# call cross entory loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# initialize optimizer adam with learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2a85c48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900.0"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3600/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "630f4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 450/450 [37:20<00:00,  4.98s/it]\n",
      "Validation:  98%|█████████▊| 62/63 [02:32<00:02,  2.45s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (8) at non-singleton dimension 0.  Target sizes: [4, -1, -1].  Tensor sizes: [8, 492, 1024]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[306], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m B, T \u001b[38;5;241m=\u001b[39m val_input_texts\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    104\u001b[0m B_y, T_y \u001b[38;5;241m=\u001b[39m val_target_tokens\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 105\u001b[0m keys_embed \u001b[38;5;241m=\u001b[39m \u001b[43mkeys_embed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m pos_encod_x \u001b[38;5;241m=\u001b[39m pos_encoding[:T]\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    107\u001b[0m pos_encod_y \u001b[38;5;241m=\u001b[39m pos_encoding[:T_y\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexpand(B_y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (8) at non-singleton dimension 0.  Target sizes: [4, -1, -1].  Tensor sizes: [8, 492, 1024]"
     ]
    }
   ],
   "source": [
    "# initilize train loss\n",
    "train_losses = []\n",
    "# initialize train acc\n",
    "train_accuracies = []\n",
    "# initialize val loss\n",
    "val_losses = []\n",
    "# initialize val acc\n",
    "val_accuracies = []\n",
    "\n",
    "# initialize file name to save and monitor loss , acc in each epochs\n",
    "monitor_csv = '../data/kb_monitor.csv'\n",
    "# intialize best val loss inf\n",
    "best_val_loss = float('inf')  # for saving best model\n",
    "\n",
    "# open file in write mode if available else create\n",
    "with open(monitor_csv, mode='w', newline='') as f:\n",
    "    # initialize writer\n",
    "    writer = csv.writer(f)\n",
    "    # write first rows\n",
    "    writer.writerow(['epoch', 'train_loss', 'train_accuracy', 'val_loss', 'val_accuracy'])\n",
    "\n",
    "# iterate each epochs\n",
    "for epoch in range(epochs):\n",
    "    # set model in train mode\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    total_train_tokens = 0\n",
    " \n",
    "    # iterate train_loader in every batch, tqdm used for progress bar\n",
    "    for input_texts, target_tokens in tqdm(train_loader_db, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        # load input text to device, like 'cuda'\n",
    "        input_texts = input_texts.to(device)\n",
    "        # load target text to device, like 'cuda'\n",
    "        target_tokens = target_tokens.to(device)\n",
    "        B, T = input_texts.shape\n",
    "        B_y, T_y = target_tokens.shape\n",
    "        pos_encod_x = pos_encoding[:T].expand(B, -1, -1)\n",
    "        pos_encod_y = pos_encoding[:T_y+1].expand(B_y, -1, -1)\n",
    "        mask_y = mask_matrix[:T_y+1, :T_y+1].expand(B_y, -1, -1)\n",
    "        keys_embed = keys_embed.expand(B_y, -1,-1)\n",
    "        start_token_expanded = start_token.expand(target_tokens.size(0), 1)\n",
    "          \n",
    "        # concat sos token in target tokens\n",
    "        target_with_sos = torch.cat([start_token_expanded, target_tokens], dim=1)\n",
    "        \n",
    "        # make optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # make auto grad false\n",
    "        with torch.no_grad():\n",
    "            # get input embed\n",
    "            input_embeds = embed_model(input_texts)['last_hidden_state'].detach()\n",
    "            # here we get embed of target token one by one, not using contextual embed , here is problem i face from few day now it solve\n",
    "            target_embeds = [embed_model(i.unsqueeze(0).unsqueeze(0))['last_hidden_state'].detach()[0][0]  for target_tokens in target_with_sos for i in target_tokens]\n",
    "            target_embeds = torch.stack(target_embeds)\n",
    "            target_embeds = target_embeds.reshape(B_y, T_y+1, 1024)\n",
    "\n",
    "\n",
    "        logits = model(x=input_embeds, y=target_embeds, mask_mat=mask_y, positional_encod_x=pos_encod_x, pos_encod_y=pos_encod_y, new2old=new2old, kb_keys=keys_embed)\n",
    "        logits = logits[:, :-1, : ]\n",
    "        \n",
    "        # find loss between final logits and target token without sos tokens \n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), target_tokens.view(-1))\n",
    "        # backward calculates the gradients of the loss with respect to all the parameters (weights and biases) in the model that have requires_grad=True\n",
    "        loss.backward()\n",
    "        # update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # loss.item() to get only value not tensor\n",
    "        total_train_loss += loss.item()\n",
    "        # it gives max logits idx that is also token ids\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        # compare pred equals to target token or not\n",
    "        total_train_correct += (preds == target_tokens).float().sum().item()\n",
    "        # numel() help to calculate total number of elements\n",
    "        total_train_tokens += target_tokens.numel()\n",
    "    \n",
    "    # it gives avg train loss\n",
    "    avg_train_loss = total_train_loss / len(train_loader_db)\n",
    "    # it gives train acc\n",
    "    train_accuracy = total_train_correct / total_train_tokens\n",
    "    \n",
    "    # append avg loss , train acc into avg_train_loss, train_accuracy\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # set model in eval mode\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_tokens = 0\n",
    "\n",
    "    # set required grad false\n",
    "    with torch.no_grad():\n",
    "        # iterate val_loader in every batch, tqdm used for progress bar\n",
    "        # all function works same as train without grad\n",
    "        for val_input_texts, val_target_tokens in tqdm(val_loader_db, desc=\"Validation: \"):\n",
    "\n",
    "            val_input_texts = val_input_texts.to(device)\n",
    "            val_target_tokens = val_target_tokens.to(device)\n",
    "\n",
    "            B, T = val_input_texts.shape\n",
    "            B_y, T_y = val_target_tokens.shape\n",
    "            keys_embed = keys_embed.expand(B_y, -1,-1)\n",
    "            pos_encod_x = pos_encoding[:T].expand(B, -1, -1)\n",
    "            pos_encod_y = pos_encoding[:T_y+1].expand(B_y, -1, -1)\n",
    "            mask_y = mask_matrix[:T_y+1, :T_y+1].expand(B_y, -1, -1)\n",
    "           \n",
    "            val_start_token_expanded = start_token.expand(val_target_tokens.size(0), 1)\n",
    "            val_target_with_sos = torch.cat([val_start_token_expanded, val_target_tokens], dim=1)\n",
    "\n",
    "            val_input_embeds = embed_model(val_input_texts)['last_hidden_state'].detach()\n",
    "            val_target_embeds = [embed_model(i.unsqueeze(0).unsqueeze(0))['last_hidden_state'].detach()[0][0]  for val_target_tokens in val_target_with_sos for i in val_target_tokens]\n",
    "            val_target_embeds = torch.stack(val_target_embeds)\n",
    "            val_target_embeds = val_target_embeds.reshape(B_y, T_y+1, 1024)\n",
    "\n",
    "            val_logits = model(x=val_input_embeds, y=val_target_embeds, mask_mat=mask_y, positional_encod_x=pos_encod_x, pos_encod_y=pos_encod_y, new2old=new2old, kb_keys=keys_embed)\n",
    "            val_logits = val_logits[:, :-1, : ]\n",
    "            val_loss = criterion(val_logits.reshape(-1, val_logits.size(-1)), val_target_tokens.view(-1))\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            val_preds = torch.argmax(val_logits, dim=-1)\n",
    "            total_val_correct += (val_preds == val_target_tokens).float().sum().item()\n",
    "            total_val_tokens += val_target_tokens.numel()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader_db)\n",
    "    val_accuracy = total_val_correct / total_val_tokens\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"../models/best_model_weight_kb.pth\")\n",
    "\n",
    "    # save current model checkpoint\n",
    "    torch.save(model.state_dict(), f\"../models/model_weight_kb_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    # log all metrics\n",
    "    with open(monitor_csv, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, train_accuracy, avg_val_loss, val_accuracy])\n",
    "    # print every epoch loss and acc\n",
    "    print(f\"Epoch {epoch + 1}: \"\n",
    "          f\"Train Loss = {avg_train_loss:.4f}, Train Acc = {train_accuracy:.4f} | \"\n",
    "          f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "0f0beea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 152161])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "46372169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 11])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6b5841c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 152161])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.view(-1, logits.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3e72a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_0 = logits[:, :-1, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "3d0f6328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 152161])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_0.reshape(88, logits_0.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adda049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "226b8477",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[281], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlogits_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "logits_0.view(-1, logits.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "7bacd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(logits_0.reshape(88, logits_0.size(-1)), target_tokens.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5ebb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e55028",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaskAttention.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# initialize model with (embed_model, vocab-size, hidden dim , embedding )\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, num_heads, embed_dim, vocab_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m StackEncoder(num_heads, embed_dim, embed_dim)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, vocab_size)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m, in \u001b[0;36mDecoderStack.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      5\u001b[0m         Decoder(embed_dim, num_heads) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_attention \u001b[38;5;241m=\u001b[39m \u001b[43mMaskMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1 \u001b[38;5;241m=\u001b[39m LayerNormalization(embed_dim)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention \u001b[38;5;241m=\u001b[39m MultiheadCrossAttention(embed_dim, num_heads)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mMaskMultiheadAttention.__init__\u001b[1;34m(self, num_heads, embed_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      8\u001b[0m     MaskAttention(embed_dim,embed_dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mMaskAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: MaskAttention.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# get vocabsize\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "# intialize num of head\n",
    "num_head = 8\n",
    "# initialize embed_dim\n",
    "embed_dim = 1024\n",
    "# initialize model with (embed_model, vocab-size, hidden dim , embedding )\n",
    "model = TransformerBlock(num_heads=num_head, embed_dim=embed_dim, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "512853e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MaskAttention.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[74], line 5\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, num_heads, embed_dim, vocab_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m StackEncoder(num_heads, embed_dim, embed_dim)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embed_dim, vocab_size)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m, in \u001b[0;36mDecoderStack.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      5\u001b[0m         Decoder(embed_dim, num_heads) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m      6\u001b[0m     ])\n",
      "Cell \u001b[1;32mIn[38], line 4\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, embed_dim, num_heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, num_heads):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_attention \u001b[38;5;241m=\u001b[39m \u001b[43mMaskMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_1 \u001b[38;5;241m=\u001b[39m LayerNormalization(embed_dim)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention \u001b[38;5;241m=\u001b[39m MultiheadCrossAttention(embed_dim, num_heads)\n",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m, in \u001b[0;36mMaskMultiheadAttention.__init__\u001b[1;34m(self, num_heads, embed_dim)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m      8\u001b[0m     MaskAttention(embed_dim,embed_dim) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_head_attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mMaskAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_heads \u001b[38;5;241m*\u001b[39m embed_dim, embed_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: MaskAttention.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "TransformerBlock(num_heads=num_head, embed_dim=embed_dim, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383daeaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
